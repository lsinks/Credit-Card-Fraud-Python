---
title: "Credit Card Fraud: Python Tutorial"
description: "An Imbalanced Class Problem"
twitter-card:
  image: "thumbnail.png"
author:
  - name: Louise E. Sinks
    url: https://lsinks.github.io/
date: 07-26-2023
categories: [Python, Machine Learning, classifiers] # self-defined categories
citation:
  url: https://lsinks.github.io/posts/
image: "thumbnail.png"
draft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!
format: html
editor: visual
---

# 1. Classification using sck-kit-learn

I will walk through a classification problem from importing the data, cleaning, exploring, fitting, choosing a model, and finalizing the model.

I wanted to create a project that could serve as a template for other two-class classification problems.

In addition to providing a template for the machine learning portion, I wanted to create nice figures and tables that could also be re-used.

Please feel free to copy and use any of my code in your work. I'd appreciate an acknowledgment or link back if you find this tutorial useful.

# 2. The problem: predicting credit card fraud

The goal of the project is to correctly predict fraudulent credit card transactions.

The specific problem is one provided by Datacamp as a challenge in the certification community. The dataset (Credit Card Fraud) can also be found at the Datacamp workspace. To access the dataset and the data dictionary, you can create a new notebook on datacamp using the Credit Card Fraud dataset. That will produce a notebook like [this](https://app.datacamp.com/workspace/w/f3a94059-683b-4bc6-b354-9b98cf3d5242/edit) with the dataset and the data dictionary.

The original source of the data (prior to preparation by DataCamp) can be found [here](https://www.kaggle.com/kartik2112/fraud-detection?select=fraudTrain.csv).

# 3. Set-up steps

Loading the necessary libraries.

```{python}
#| label: loading-libraries
#| warning: false

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from skimpy import skim

```

Setting a palette for seaborn. I like viridis and it is color-blind friendly.

```{python}
#| label: fig-options

sns.color_palette("viridis", as_cmap=True)
```

Loading the data. This is a local copy that is part of the workspace download from Datacamp.

```{python}
#| label: import-data

fraud = pd.read_csv("credit_card_fraud.csv", parse_dates = ["dob", "trans_date_trans_time"]) 

```

Looking at the data

```{python}
#| label: view-data
fraud.head()

```

# 4. Validation of data types

I examine the dataset via `skim` and make sure all data elements are as expected. `skim` is a function in the [skimr package](https://cran.r-project.org/web/packages/skimr/index.html) that provides a high-level summary of the data. The output is a dataframe, [so it can be manipulated and formatted more nicely](https://lsinks.github.io/posts/2023-03-24-tidytuesday-figure-polishing/#skimr-to-understand-your-data) than the output of `summary()`.

```{python}
#| label: skim-data

skim(fraud)
```

Everything looks okay, and I am lucky because there is no missing data. I will not need to do cleaning or imputation.

I see that `is_fraud` is coded as 0 or 1, and the mean of this variable is 0.00525. The number of fraudulent transactions is very low, and we should use treatments for imbalanced classes when we get to the fitting/ modeling stage.

# 5. Do all variables have sensible types?

I will look at each variable and decide whether to keep, transform, or drop it. This is a mixture of Exploratory Data Analysis and Feature Engineering, but I find it helpful to do some simple feature engineering as I start exploring the data. In this project, we have all data to begin with, so any transformations will be performed on the entire dataset.

Questions to consider:

-   Should strings be converted to factors?
-   Is date-time data properly encoded?
-   Is financial data encoded numerically?
-   Is geographic data consistently rendered? (city/ state strings vs. lat/long numeric pairs)

First, I grouped all my variables by type and examined each variable class by class. The dataset has the following types of variables:

1.  Strings
2.  Geospatial Data
3.  Dates
4.  Date/Times
5.  Numerical

As I go through the different classes of variables, I will provide information from the data dictionary about them.

## 5.1. Looking at the strings

Strings are usually not a useful format for classification problems. The strings should be converted to factors, dropped, or otherwise transformed.

***5.1.1. Strings to Factors*** (Code Block 6 - 8)

-   `category`, Category of Merchant
-   `job`, Job of Credit Card Holder

***5.1.2. Strings as Strings*** (Code Block 9)

-   `merchant`, Merchant Name
-   `trans_num`, Transaction Number

I'm not going to retain these, as they are either unlikely to have predictive power (`trans_num`) or are highly correlated with other predictors (`merchant` with `merch_lat`/`merch_long`.)

***5.2. Strings to Geospatial Data*** (Code Block 13)

We have plenty of geospatial data as lat/long pairs, so I want to convert city/state to lat/long so I can compare to the other geospatial variables. This will also make it easier to compute new variables like the distance the transaction is from the home location. I will transform and explore this when I handle the other geospatial data.

-   `city`, City of Credit Card Holder
-   `state`, State of Credit Card Holder

**Things to consider as we walk through the data:**

-   Do we have typos that lead to duplicate entries : VA/ Va. / Virginia?
-   Do we have excessive \# of categories? Do we want to combine some?
-   Should they be ordered?

### 5.1.1. Exploring the factors: how is the compactness of categories?

The predictors `category` and `job` are transformed into factors.

```{python}
#| label: convert-strings-to-factors

fraud["category"] = fraud["category"].astype("category")
fraud["job"] = fraud["job"].astype("category")

fraud["category"].dtype
fraud["job"].dtype

```

From the skim output, I see that `category` has 14 unique values, and `job` has 163 unique values. The dataset is quite large, with 339,607 records, so these variables don't have an excessive number of levels at first glance. However, it is worth seeing if I can compact the levels to a smaller number.

#### Why do we care about the number of categories and whether they are "excessive"?

Consider the extreme case where a dataset had categories that only contained one record each. There is simply insufficient data to make correct predictions using category as a predictor on new data with that category label. Additionally, if your modeling uses dummy variables, having an extremely large number of categories will lead to the production of a huge number of predictors, which can slow down the fitting. This is fine if all the predictors are useful, but if they aren't useful (as in the case of having only one record for a category), trimming them will improve the speed and quality of the data fitting.

If I had subject matter expertise, I could manually combine categories. For example, in this dataset, the three largest categories in `job` are surveying-related and perhaps could be combined. If you don't have subject matter expertise, or if performing this task would be too labor intensive, then you can use cutoffs based on the amount of data in a category. If the majority of the data exists in only a few categories, then it might be reasonable to keep those categories and lump everything else in an "other" category or perhaps even drop the data points in smaller categories.

Another way to evaluate the compactness is to [make a cumulative plot](https://stackoverflow.com/questions/15844919/cumulative-plot-%20using-ggplot2). This looks at the proportion of data that is described as you add categories.

```{python}
#| label: compactness



plt.clf()
fig, ax = sns.ecdfplot(data = fraud, x = "category", stat = 'proportion', color = "darkcyan")
ax.set_title("Exploring Categorical Variables")
plt.xticks(rotation=45)

plt.show()
```

If you look at Figure 1A, roughly 75-80 categories have to be included to capture 80% of the data. For Figure 1B, roughly ten categories have to be included. Ideally, you'd like a very steep curve initially (where a "small number" of categories cover the "majority" of the data) and then a long, shallow tail approaching 100% that corresponds to the data to be binned in "other" or dropped. There aren't hard and fast rules on making these decisions. I decided to use 80% as my threshold. Both of these curves look relatively shallow to me, so I decided not to do any binning, grouping, or dropping of levels.

I decided to look at all the categories of transactions just to see which ones were the most common.

```{python}
#| label: category-levels

fraud.value_counts("category", sort = True).plot(kind = "bar", color = "darkcyan")
plt.xticks(rotation=45)
plt.show()
```

Gas/transport was the most common category, and grocery was the second most common, both of which make sense. The least common category was travel. Nothing seemed unusual in the ranking.

### 5.1.2. Looking at our character strings

Merchant name (`merchant`) and transaction number(`trans_num`) are both strings. Transaction number should not influence fraud rate as it is a number assigned to the transaction when processed. I will drop it from our dataset. Merchant name could be correlated with fraud, for example, if a company's employee was involved. However, this data is also represented by the location and category. If a location/category is found to have higher levels of fraud, then a more detailed examination of those transactions can be performed, including the merchant name. Here, I also remove it from the dataset.

```{python}
#| label: removing-merchant-transnum
# Code Block 9: Removing Character/ String Variables
fraud = fraud.drop(columns = ["merchant","trans_num"])
```

## 5.2. Looking at the geographic data

This data is coded as numeric (latitude and longitude) or character (city/state), but we can recognize it as geographic data and treat it appropriately.

First, there are two sets of geographic data related to the merchant. The location of the merchant and where the transaction occurred. I create scatter plots of latitude and longitude separately, because I want to check the correlation between the two sources of data (merchant and transaction). I create a shared legend following the article [here](https://wilkelab.org/cowplot/articles/shared_legends.html).

```{python}
#| label: transaction-merchant-coords
# Code Block 10: Comparing Merchant and Transaction Locations

# calculate correlations
fraud.corr(numeric_only=True)
#cor_long = fraud.corr("long", "merch_long")

#cor_lat
#cor_long
plt.clf()
sns.pairplot(fraud, vars = ["long", "merch_long"])
plt.show()

plt.clf()
sns.pairplot(fraud, vars = ["lat", "merch_lat"])
plt.show()
```

```{python}
#| label: drop-all-geographic-vars
fraud = fraud.drop(columns = ["lat", "long", "city", "state"])

```

## 5.3. Looking at the dates

**Date**

`dob`, Date of Birth of Credit Card Holder

Questions:

-   What is the date range, and does it make sense?

-   Do we have improbably old or young people?

-   Do we have historic or futuristic transaction dates?

I calculate the `age` from the `dob` and visualize them both.

```{python}
#| label: dob-viz
# Code Block 17: Looking at dob

plt.clf()
sns.histplot(x = "dob", data = fraud, bins = 10)
plt.show()

```

```{python}
#| label: calculating-plotting-age
#first transaction
start_date = fraud["trans_date_trans_time"].min()
start_date
fraud["age"] = np.floor((start_date - fraud['dob'])/np.timedelta64(1,'Y'))

plt.clf()
sns.histplot(x = "age", data = fraud, bins = 10)
plt.show()
```

These don't match the R analysis.

```{python}
fraud.value_counts("age", sort = False)

```

The head and tail values match, but the ones in the middle don't.

The ages seem reasonable (calculated relative to the earliest date of transactions). There are a few thousand 17-year-olds, which is too young to have their own credit card, but it is plausible that they would be an authorized user on their parents' card. `age` seems a more reasonable variable than `dob`, so `dob` is also dropped from the dataset. For example, scammers might be more likely to target 90-year-olds. The age is the feature that leads to them being targeted, not the birth year. The birth year is related to age through the current date- in 10 years, a new cohort of birth years would be targeted if age is the important feature. So the `age` feature is more robust to passing time than `dob`.

```{python}
# Code Block 18: Removing dob

fraud = fraud.drop(columns = ["dob"])

```

## 5.4. Looking at the date-times

**date-time**

`trans_date_trans_time`, Transaction DateTime

**Questions**

Would processing the date-times yield more useful predictors?

First, I want to look at variation in the number of transactions with date-time. I chose to use a histogram with bins corresponding to one month widths.

```{python}
plt.clf()
sns.histplot(x= "trans_date_trans_time", data = fraud, bins = 24)
plt.xticks(rotation=45)
plt.show()
```

Next, I will break the transaction date-time into day of the week and hour.

```{python}
#| label: day-transactions
# Code Block 20: 

#fraud["day"] = datetime.weekday(fraud["trans_date_trans_time"])
#hours = fraud["trans_date_trans_time"].hour
fraud["day"] = fraud["trans_date_trans_time"].dt.weekday
fraud["hour"] = fraud["trans_date_trans_time"].dt.hour

plt.clf()
sns.histplot(x= "day", data= fraud, bins = 7)
plt.show()

fraud.value_counts("day")
```

Monday has the highest number of transactions; this could be due to businesses processing orders that came in over the weekend. By default, pandas codes the day of the week as a number where 0 means Monday, 6 means Sunday.

Now, I look at what time of day do most transactions occur?

```{python}
#| label: hour-transactions-graph
# Code Block 21: What time do transactions occur
plt.clf()
sns.histplot(x = "hour", data = fraud, bins = 24)
plt.show()

```

This data honestly looks funny to me. I might expect that most transactions would occur during normal business hours (\~9-5) or more occur during lunch or after work, but what we see is a lower number of transactions from midnight to \~ 2 pm and then a higher number of transactions from 2 pm until midnight. The odd pattern could be a sign that something is wrong with the data (perhaps timezones aren't being encoded properly?), or it could be simply a lack of subject matter knowledge (for example, transactions are pre-authorized at the time of sale and processed later, and the transaction time is the processing time, not the sale time.) Of course, this is also a synthetic dataset, so this pattern may be simply the result of user input choices when the set was generated. If this were a real dataset, I'd chase this down.

```{python}
#| label: remove-transdatetranstime
# Code Block 23:
#removing the original variable and keeping the component variables.
fraud = fraud.drop(columns =["trans_date_trans_time"])
```

## 5.5. Looking at the numerical variables

**Numerical**

`amt`, transaction amount

**Questions**

Would transforming this data produce a more normal distribution?

Generally, more normal or at least more symmetric data tends to be fitted better, especially when using model-fitting algorithms that arise from statistics rather than pure machine learning.

I compare the original data with the log-transformed data.

```{python}
#| label: amt-log-amt-graph

fraud["log_amt"] = np.log(fraud["amt"])

plt.clf()
sns.histplot(x= "amt", data = fraud, bins = 50)
plt.show()

```

now the other one.

```{python}
plt.clf()
sns.histplot(x= "log_amt", data = fraud, bins = 50)
plt.show()
```

The transformed data is more symmetric so that the transformed variable will be retained.

```{python}
#| label: log-amt-feature
# Code Block 25:
fraud = fraud.drop(columns = ["amt", "job"])

```

I do a final clean-up of variables next. I remove some variables that I don't think will impact fraud- the population of the home city and the location of the home. I don't think the home should have an impact on fraud; it is where the card is used, not where it is billed, that should matter. I suppose you could have a neighborhood where all the mail was being stolen, and cards were compromised that way, but I think most cards get compromised at the point of sale.

```{python}
fraud = fraud.drop(columns = ["city_pop"])
```

I hate python

```{python}
fraud = fraud.drop(columns = ["merch_lat", "merch_long"])
```

# 6. Final preparation for modeling

Next, I plot the correlation plot for the dataset. Highly correlated variables can cause problems for some fitting algorithms, again, especially for those coming from statistics. It also gives you a bit of a feel for what might come out of the model fitting. This is also a chance to do one last fact-check. For example, `category` and `amt` are reasonably correlated. The sign isn't particularly important in this case since `category` is arbitrarily ordered.

instructions for lower diagonal plot here https://seaborn.pydata.org/examples/many_pairwise_correlations.html

```{python}
#| label: correlation-graph
#Code Block 27: examining correlation between variables 
plt.clf()
sns.heatmap(fraud.corr(numeric_only=True), cmap="YlGnBu", annot=True)
plt.show()
```

And take one last look at the data and make sure I have the variables I expect.

```{python}
#| label: final-check-of-data
# Code Block 29: Viewing Final Fraud Dataset
fraud.info()

```

# Just a simple Log Reg with no hyper

```{python}
#fraud_dummies = pd.get_dummies(fraud, drop_first= True)
#X = fraud_dummies.drop("is_fraud", axis = 1) # Features
#y = fraud_dummies["is_fraud"] # Target variable
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler,LabelBinarizer, OneHotEncoder
from sklearn.pipeline import Pipeline

X = fraud.drop("is_fraud", axis = 1) # Features
y = fraud["is_fraud"] # Target variable

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.25, random_state=8675309 )


numeric_features=list(X.select_dtypes('number').columns)
categorical_features=list(X.select_dtypes('category').columns)

numeric_transformer = Pipeline(steps = ["scaler", StandardScaler()])
categorical_transformer = Pipeline(steps = ["onehot", OneHotEncoder()])


preprocessor = ColumnTransformer(transformers = [("num", numeric_transformer, numeric_features), ("cat", categorical_transformer, categorical_features)])



```

Pre-process

```{python}


rf = Pipeline(steps = [("preprocessor", preprocessor),
     ("classifier", RandomForestClassifier())])

rf

```

```{python}
import numpy as np

from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler

numeric_preprocessor = Pipeline(
    steps=[
        ("imputation_mean", SimpleImputer(missing_values=np.nan, strategy="mean")),
        ("scaler", StandardScaler()),
    ]
)

categorical_preprocessor = Pipeline(
    steps=[
        (
            "imputation_constant",
            SimpleImputer(fill_value="missing", strategy="constant"),
        ),
        ("onehot", OneHotEncoder(handle_unknown="ignore")),
    ]
)

preprocessor = ColumnTransformer(
    [
        ("categorical", categorical_preprocessor, categorical_features),
        ("numerical", numeric_preprocessor, numeric_features),
    ]
)

 
pipe = make_pipeline(preprocessor, RandomForestClassifier())
#pipe = make_pipeline(preprocessor, LogisticRegression(max_iter=500))
pipe  # click on the diagram below to see the details of each step
```

This code is copeid from scikitlearn itself https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_pipeline_display.html#displaying-a-pipeline-chaining-multiple-preprocessing-steps-classifier

now the split

```{python}
pipe.fit(X_train, y_train)  # apply scaling on training data
y_pred = pipe.predict(X_test)

from sklearn.metrics import balanced_accuracy_score
bal_acc=balanced_accuracy_score(y_test,y_pred, adjusted=True )
```

now the fit

```{python}

```

# 7. Finding a high performing model

(https://towardsdatascience.com/quickly-test-multiple-models-a98477476f0)

I'm planning to study the following models and methods of handling imbalanced class problems.

Explore different classification models

1.  logistic regression

2.  elastic net logistic regression

3.  lightgbm

4.  random forest

Explore different method of handling imbalanced class problems

1.  do nothing

2.  SMOTE

3.  ROSE

4.  downsample

This ends up being 4 x 4 different fits, and keeping track of all the combinations can become difficult. Luckily, tidymodels has a function workflow_set that will create all the combinations and workflow_map to run all the fitting procedures.

TOOK CATEgory out and need to put it back later

```{python}


#new_cols = fraud_dummies.columns[~fraud_dummies.columns.isin(fraud.columns)]


```

New cols has the info, but I don't know how to join it with the columns.

```{python}
#feature_cols = ["age", "day", "hour", "log_amt", "category_entertainment", 'category_food_dining',
       'category_gas_transport', 'category_grocery_net',
       'category_grocery_pos', 'category_health_fitness', 'category_home',
       'category_kids_pets', 'category_misc_net', 'category_misc_pos',
       'category_personal_care', 'category_shopping_net',
       'category_shopping_pos', 'category_travel']


```

make a smaller dataset for testing.

```{python}
fraud_small =fraud_dummies.groupby('is_fraud', group_keys=False).apply(lambda x: x.sample(200))
```

```{python}

#feature_cols = ["age", "day", "hour", "log_amt"]
#X = fraud_small[feature_cols] # Features
#y = fraud_small["is_fraud"] # Target variable

#X = fraud_dummies.drop("is_fraud", axis = 1) # Features
#y = fraud_dummies["is_fraud"] # Target variable

```

## 7.1. Splitting the data

```{python}


```

First, preparation work. Here, I split the data into a testing and training set. I also create folds for cross-validation from the training set. (Added stratify. Not sure it works.)

```{python}
#| label: splits-and-folds
# Code Block 30 : Train/Test Splits & CV Folds 

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.25, random_state=8675309 )

```

## 7.2. Creating recipes

Next, I create recipes that do preprocessing of the data- making dummy variables, normalizing, and removing variables that only contain one value (`step_zv(all_predictors())`). The processing will be applied to both the training and testing data as you move through the workflow.

I used the chart found in [Appendix A](https://www.tmwr.org/pre-proc-table.html) of the Tidy Modeling with R by Max Kuhn and Julia Silge to choose the preprocessing of data. Some models require specific types of preprocessing, others don't require it, but it can produce better or faster fitting, and in other cases, the preprocessing isn't required and probably doesn't help. The chart breaks this down for each category of preprocessing model by model. The same preprocessing steps were required or recommended for the models I chose, so I used them across the board. You can create recipes for different models and build a workflow manually to match the models to the proper recipe. This process is covered extensively in [Chapter 15](https://www.tmwr.org/workflow-sets.html) of Tidy Modeling with R.

I use the selector functions (`all_nominal_predictors()`, `all_numerical_predictors()`, etc.) available in the tidymodels framework. A listing of all selector functions usable in tidymodels can be found [here](https://recipes.tidymodels.org/reference/selections.htm). Using selector functions when handling groups of features reduces the chance of mistakes and typos.

I then modify this recipe to handle the imbalanced class problem. I use SMOTE and ROSE hybrid methods to balance the classes. These methods create synthetic data for the minority class and downsample the majority class to balance the classes. I also use downsample, which throws away majority class records to balance the two classes. A good overview is [here](https://www.r-bloggers.com/2019/04/methods-for-dealing-with-imbalanced-data/), and it also provides a tutorial for handling this type of problem with caret, rather than tidymodels. These recipe steps require the [themis package](https://cran.r-project.org/web/packages/themis/index.html).

```{python}
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
#from xgboost import XGBClassifier
from sklearn import model_selection
from sklearn.utils import class_weight
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
```

Try a single regression

```{python}
# import the class
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

pipe = make_pipeline(StandardScaler(), LogisticRegression())
pipe.fit(X_train, y_train)  # apply scaling on training data


pipe.score(X_test, y_test) 


```

how did we do?

```{python}

```

```{python}
#| label: creating-recipes
# 
# 
# def run_exps(X_train: pd.DataFrame , y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame) -> pd.DataFrame:
#     '''
#     Lightweight script to test many models and find winners
# :param X_train: training split
#     :param y_train: training target vector
#     :param X_test: test split
#     :param y_test: test target vector
#     :return: DataFrame of predictions
#     '''
#     
#     dfs = []
# models = [
#           ('LogReg', LogisticRegression()), 
#           ('RF', RandomForestClassifier()),
#           ('KNN', KNeighborsClassifier()),
#           ('SVM', SVC()), 
#           ('GNB', GaussianNB()),
#         ]
# results = []
#     names = []
#     scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted', 'roc_auc']
#     target_names = ['malignant', 'benign']
# for name, model in models:
#         kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=90210)
#         cv_results = model_selection.cross_validate(model, X_train, y_train, cv=kfold, scoring=scoring)
#         clf = model.fit(X_train, y_train)
#         y_pred = clf.predict(X_test)
#         print(name)
#         print(classification_report(y_test, y_pred, target_names=target_names))
# results.append(cv_results)
#         names.append(name)
# this_df = pd.DataFrame(cv_results)
#         this_df['model'] = name
#         dfs.append(this_df)
# final = pd.concat(dfs, ignore_index=True)
# return final
```

## 7.3. Setting the model engines

Next, I set the engines for the models. I tune the hyperparameters of the elastic net logistic regression and the lightgbm. Random Forest also has tuning parameters, but the random forest model is pretty slow to fit, and adding tuning parameters makes it even slower. If none of the other models worked well, then tuning RF would be a good idea.

```{python}
#| label: setting-engines
bootstraps = []
for model in list(set(final.model.values)):
    model_df = final.loc[final.model == model]
    bootstrap = model_df.sample(n=30, replace=True)
    bootstraps.append(bootstrap)
        
bootstrap_df = pd.concat(bootstraps, ignore_index=True)
results_long = pd.melt(bootstrap_df,id_vars=['model'],var_name='metrics', value_name='values')
time_metrics = ['fit_time','score_time'] # fit time metrics
## PERFORMANCE METRICS
results_long_nofit = results_long.loc[~results_long['metrics'].isin(time_metrics)] # get df without fit data
results_long_nofit = results_long_nofit.sort_values(by='values')
## TIME METRICS
results_long_fit = results_long.loc[results_long['metrics'].isin(time_metrics)] # df with fit data
results_long_fit = results_long_fit.sort_values(by='values')
```

## 7.4. Creating a metrics set

Lastly, I create a metrics set in Code Block 33. Accuracy is generally a terrible metric for highly imbalanced problems; the model can achieve high accuracy by assigning everything to the majority class. Alternate metrics like [sensitivity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) or [j-index](https://en.wikipedia.org/wiki/Youden%27s_J_statistic) are better choices for the imbalanced class situation.

```{r}
#| label: metrics-set
# Code Block 33: Setting Metrics

fraud_metrics <-
  metric_set(roc_auc, accuracy, sensitivity, specificity, j_index)
```

## 7.5. Creating the workflow_set

Next, I create the workflow_set. This is where tidymodels shines. I feed it the 4 recipes and the 4 engines, and it makes all the permutations to fit. (As I mentioned earlier, you can manually create a workflow_set where you assign specific recipes to specific models, but here all recipes work with all models.)

```{python}
#| label: workflowset
import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(20, 12))
sns.set(font_scale=2.5)
g = sns.boxplot(x="model", y="values", hue="metrics", data=results_long_nofit, palette="Set3")
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
plt.title('Comparison of Model by Classification Metric')
plt.savefig('./benchmark_models_performance.png',dpi=300)
```

dshjkg

```{python}
import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(20, 12))
sns.set(font_scale=2.5)
g = sns.boxplot(x="model", y="values", hue="metrics", data=results_long_nofit, palette="Set3")
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
plt.title('Comparison of Model by Classification Metric')
plt.savefig('./benchmark_models_performance.png',dpi=300)
```

fdddfs

```{python}
metrics = list(set(results_long_nofit.metrics.values))
bootstrap_df.groupby(['model'])[metrics].agg([np.std, np.mean])
```

asdds

```{python}
time_metrics = list(set(results_long_fit.metrics.values))
bootstrap_df.groupby(['model'])[time_metrics].agg([np.std, np.mean])
```
