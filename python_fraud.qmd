---
title: "Credit Card Fraud: Python Tutorial"
description: "An Imbalanced Class Problem"
twitter-card:
  image: "thumbnail.png"
author:
  - name: Louise E. Sinks
    url: https://lsinks.github.io/
date: 07-26-2023
categories: [Python, Machine Learning, classifiers] # self-defined categories
citation:
  url: https://lsinks.github.io/posts/
image: "thumbnail.png"
draft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!
format: html
editor: visual
---

# 1. Classification using sck-kit-learn

I will walk through a classification problem from importing the data, cleaning, exploring, fitting, choosing a model, and finalizing the model.

I wanted to create a project that could serve as a template for other two-class classification problems.

In addition to providing a template for the machine learning portion, I wanted to create nice figures and tables that could also be re-used.

Please feel free to copy and use any of my code in your work. I'd appreciate an acknowledgment or link back if you find this tutorial useful.

# 2. The problem: predicting credit card fraud

The goal of the project is to correctly predict fraudulent credit card transactions.

The specific problem is one provided by Datacamp as a challenge in the certification community. The dataset (Credit Card Fraud) can also be found at the Datacamp workspace. To access the dataset and the data dictionary, you can create a new notebook on datacamp using the Credit Card Fraud dataset. That will produce a notebook like [this](https://app.datacamp.com/workspace/w/f3a94059-683b-4bc6-b354-9b98cf3d5242/edit) with the dataset and the data dictionary.

The original source of the data (prior to preparation by DataCamp) can be found [here](https://www.kaggle.com/kartik2112/fraud-detection?select=fraudTrain.csv).

# 3. Set-up steps

Loading the necessary libraries.

```{python}
#| label: loading-libraries
#| warning: false

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from skimpy import skim
```

Setting a palette for seaborn. I like viridis and it is color-blind friendly.

```{python}
#| label: fig-options

sns.color_palette("viridis", as_cmap=True)
```

Loading the data. This is a local copy that is part of the workspace download from Datacamp.

```{python}
#| label: import-data

fraud = pd.read_csv("credit_card_fraud.csv", parse_dates = ["dob", "trans_date_trans_time"]) 

```

Looking at the data

```{python}
#| label: view-data
fraud.head()
```

# 4. Validation of data types

I examine the dataset via `skim` from `skimpy` and make sure all data elements are as expected. This package was inspired by the r package `skimr`. I find `skimpy`/`skimr` give a richer output than `describe()` or `summary()`.

This looks nicer when rendered than in the console.

```{python}
#| label: skim-data

skim(fraud)
```

Everything looks okay, and I am lucky because there is no missing data. I will not need to do cleaning or imputation.

I see that `is_fraud` is coded as 0 or 1, and the mean of this variable is 0.0052. The number of fraudulent transactions is very low, and I should use treatments for imbalanced classes when we get to the fitting/ modeling stage.

# 5. Do all variables have sensible types?

I will look at each variable and decide whether to keep, transform, or drop it. This is a mixture of Exploratory Data Analysis and Feature Engineering, but I find it helpful to do some simple feature engineering as I start exploring the data. In this project, we have all data to begin with, so any transformations will be performed on the entire dataset.

Questions to consider:

-   Should strings be converted to factors?
-   Is date-time data properly encoded?
-   Is financial data encoded numerically?
-   Is geographic data consistently rendered? (city/ state strings vs. lat/long numeric pairs)

First, I grouped all my variables by type and examined each variable class by class. The dataset has the following types of variables:

1.  Strings
2.  Geospatial Data
3.  Dates
4.  Date/Times
5.  Numerical

As I go through the different classes of variables, I will provide information from the data dictionary about them.

## 5.1. Looking at the strings

Strings are usually not a useful format for classification problems. The strings should be converted to factors, dropped, or otherwise transformed.

***5.1.1. Strings to Factors***

-   `category`, Category of Merchant
-   `job`, Job of Credit Card Holder

***5.1.2. Strings as Strings*** (Code Block 9)

-   `merchant`, Merchant Name
-   `trans_num`, Transaction Number

I'm not going to retain these, as they are either unlikely to have predictive power (`trans_num`) or are highly correlated with other predictors (`merchant` with `merch_lat`/`merch_long`.)

***5.2. Strings to Geospatial Data*** (Code Block 13)

We have plenty of geospatial data as lat/long pairs, so I want to convert city/state to lat/long so I can compare to the other geospatial variables. This will also make it easier to compute new variables like the distance the transaction is from the home location. I will transform and explore this when I handle the other geospatial data.

-   `city`, City of Credit Card Holder
-   `state`, State of Credit Card Holder

**Things to consider as we walk through the data:**

-   Do we have typos that lead to duplicate entries : VA/ Va. / Virginia?
-   Do we have excessive \# of categories? Do we want to combine some?
-   Should they be ordered?

### 5.1.1. Exploring the factors: how is the compactness of categories?

The predictors `category` and `job` are transformed into factors.

```{python}
#| label: convert-strings-to-factors

fraud["category"] = fraud["category"].astype("category")
print("Category: ")
fraud["category"].nunique()

fraud["job"] = fraud["job"].astype("category")
print("Job: ")
fraud["job"].nunique()


```

I see that `category` has 14 unique values, and `job` has 163 unique values. The dataset is quite large, with 339,607 records, so these variables don't have an excessive number of levels at first glance. However, it is worth seeing if I can compact the levels to a smaller number.

I don't want to repeat what I said when I first did this analysis, so please see BLAH for more detail.

Another way to evaluate the compactness is to [make a cumulative plot](https://stackoverflow.com/questions/15844919/cumulative-plot-%20using-ggplot2). This looks at the proportion of data that is described as you add categories.

```{python}
#| label: compactness
plt.clf()
ax = sns.ecdfplot(data = fraud, x = "category", stat = 'proportion', color = "darkcyan")
ax.set_title("Exploring Categorical Variables")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

The ecdf plot prints with the categories in alpha order. I'd like them in size order.

```{python}
fraud.value_counts("category", sort = True)

```

HOW DO I DO THIS WITHOUT TYPING EVERYTHING IN. I HATE PANDAs.

```{python}
my_order = ["gas_transport", "grocery_pos", "home", "shopping_pos", "kids_pets", "shopping_net", "personal_care","entertainment","food_dining","health_fitness",  
"misc_pos","misc_net", "grocery_net","travel"]

#dfs["A"] = dfs["A"].cat.reorder_categories(["a", "b", "e"])
fraud["category_new"] = fraud["category"].cat.reorder_categories(my_order, ordered = True)

```

blah

```{python}
fraud["category_new"]
```

Now try the edcf plot

```{python}
ax = sns.ecdfplot(data = fraud, x = "category_new", stat = 'proportion', color = "darkcyan")
ax.set_title("Exploring Categorical Variables")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

80 PERCENT THRESHOLD AND THE JOBS PLOT. Why does it only show 7 categories

```{python}
plt.clf()
fraud.value_counts("category", sort = True).cumsum().plot(color= "darkcyan")
ax.set_title("Exploring Categorical Variables")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

```{python}
#| label: category-levels
plt.clf()
fraud.value_counts("category_new").plot(kind = "bar", color = "darkcyan")
plt.xticks(rotation=45)
plt.show()
```

Gas/transport was the most common category, and grocery was the second most common, both of which make sense. The least common category was travel. Nothing seemed unusual in the ranking.

### 5.1.2. Looking at our character strings

Merchant name (`merchant`) and transaction number(`trans_num`) are both strings. Transaction number should not influence fraud rate as it is a number assigned to the transaction when processed. I will drop it from our dataset. Merchant name could be correlated with fraud, for example, if a company's employee was involved. However, this data is also represented by the location and category. If a location/category is found to have higher levels of fraud, then a more detailed examination of those transactions can be performed, including the merchant name. Here, I also remove it from the dataset.

```{python}
#| label: removing-merchant-transnum
# Code Block 9: Removing Character/ String Variables
fraud = fraud.drop(columns = ["merchant","trans_num"])
```

## 5.2. Looking at the geographic data

The geographic data is a confusing mess, as I discovered when I started playing with Tableau.

```{python}
#| label: transaction-merchant-coords
# Code Block 10: Comparing Merchant and Transaction Locations

# calculate correlations
#fraud.corr(numeric_only=True)
#cor_long = fraud.corr("long", "merch_long")

#cor_lat
#cor_long
plt.clf()
sns.pairplot(fraud, vars = ["long", "merch_long"], color = "darkcyan")
plt.show()

plt.clf()
sns.pairplot(fraud, vars = ["lat", "merch_lat"], color = "darkcyan")
plt.show()
```

```{python}
#| label: drop-all-geographic-vars
fraud = fraud.drop(columns = ["lat", "long", "city", "state"])

```

## 5.3. Looking at the dates

**Date**

`dob`, Date of Birth of Credit Card Holder

Questions:

-   What is the date range, and does it make sense?

-   Do we have improbably old or young people?

-   Do we have historic or futuristic transaction dates?

I calculate the `age` from the `dob` and visualize them both.

```{python}
#| label: dob-viz
# Code Block 17: Looking at dob

plt.clf()
sns.histplot(x = "dob", data = fraud, bins = 10, color = "darkcyan")
plt.show()

```

```{python}
#| label: calculating-plotting-age
#first transaction
start_date = fraud["trans_date_trans_time"].min()
start_date
fraud["age"] = np.floor((start_date - fraud['dob'])/np.timedelta64(1,'Y'))

plt.clf()
sns.histplot(x = "age", data = fraud, bins = 10, color = "darkcyan")
plt.show()
```

These don't match the R analysis. DOB looks closer, but the age has a completely different shape. DIfference bwetween truncate and floor? I make an interval in R using lubridate rather than a simple substraction

```{python}
fraud.value_counts("age", sort = True)

```

The head and tail values match, but the ones in the middle don't.

The ages seem reasonable (calculated relative to the earliest date of transactions). There are a few thousand 17-year-olds, which is too young to have their own credit card, but it is plausible that they would be an authorized user on their parents' card. `age` seems a more reasonable variable than `dob`, so `dob` is also dropped from the dataset. For example, scammers might be more likely to target 90-year-olds. The age is the feature that leads to them being targeted, not the birth year. The birth year is related to age through the current date- in 10 years, a new cohort of birth years would be targeted if age is the important feature. So the `age` feature is more robust to passing time than `dob`.

```{python}
# Code Block 18: Removing dob

fraud = fraud.drop(columns = ["dob"])

```

## 5.4. Looking at the date-times

**date-time**

`trans_date_trans_time`, Transaction DateTime

**Questions**

Would processing the date-times yield more useful predictors?

First, I want to look at variation in the number of transactions with date-time. I chose to use a histogram with bins corresponding to one month widths. **This also doesn't match R.**

```{python}
plt.clf()
sns.histplot(x= "trans_date_trans_time", data = fraud, bins = 24)
plt.xticks(rotation=45)
plt.show()
```

Next, I will break the transaction date-time into day of the week and hour.

```{python}
#| label: day-transactions
# Code Block 20: 

#fraud["day"] = datetime.weekday(fraud["trans_date_trans_time"])
#hours = fraud["trans_date_trans_time"].hour
fraud["day"] = fraud["trans_date_trans_time"].dt.weekday
fraud["hour"] = fraud["trans_date_trans_time"].dt.hour

plt.clf()
sns.histplot(x= "day", data= fraud, bins = 7)
plt.show()

fraud.value_counts("day")
```

Monday has the highest number of transactions; this could be due to businesses processing orders that came in over the weekend. By default, pandas codes the day of the week as a number where 0 means Monday, 6 means Sunday. **This also doesn't match R. This does look like Tableau though.**

```{r}
library(tidyverse)
library(reticulate)
fraud_r <- py$fraud

```

Now

```{r}
fraud_r %>% mutate(weekday = wday(trans_date_trans_time))%>%
  group_by(weekday)%>% count(weekday)

wday(fraud_r$trans_date_trans_time[1])
fraud_r$trans_date_trans_time[1]
#this is not correct locally. It shoudl be a monday.
```

Now, I look at what time of day do most transactions occur? **(Are there two different timezones?)**

```{python}
#| label: hour-transactions-graph
# Code Block 21: What time do transactions occur
plt.clf()
sns.histplot(x = "hour", data = fraud, bins = 24, color = "darkcyan")
plt.show()

```

This data honestly looks funny to me. I might expect that most transactions would occur during normal business hours (\~9-5) or more occur during lunch or after work, but what we see is a lower number of transactions from midnight to \~ 2 pm and then a higher number of transactions from 2 pm until midnight. The odd pattern could be a sign that something is wrong with the data (perhaps timezones aren't being encoded properly?), or it could be simply a lack of subject matter knowledge (for example, transactions are pre-authorized at the time of sale and processed later, and the transaction time is the processing time, not the sale time.) Of course, this is also a synthetic dataset, so this pattern may be simply the result of user input choices when the set was generated. If this were a real dataset, I'd chase this down.

```{python}
#| label: remove-transdatetranstime
# Code Block 23:
#removing the original variable and keeping the component variables.
fraud = fraud.drop(columns =["trans_date_trans_time"])
```

## 5.5. Looking at the numerical variables

**Numerical**

`amt`, transaction amount

**Questions**

Would transforming this data produce a more normal distribution?

Generally, more normal or at least more symmetric data tends to be fitted better, especially when using model-fitting algorithms that arise from statistics rather than pure machine learning.

I compare the original data with the log-transformed data.

```{python}
#| label: amt-log-amt-graph

fraud["log_amt"] = np.log(fraud["amt"])

plt.clf()
sns.histplot(x= "amt", data = fraud, bins = 50)
plt.show()

```

now the other one.

```{python}
plt.clf()
sns.histplot(x= "log_amt", data = fraud, bins = 50)
plt.show()
```

The transformed data is more symmetric so that the transformed variable will be retained.

```{python}
#| label: log-amt-feature
# Code Block 25:
fraud = fraud.drop(columns = ["amt", "job"])

```

I do a final clean-up of variables next. I remove some variables that I don't think will impact fraud- the population of the home city and the location of the home. I don't think the home should have an impact on fraud; it is where the card is used, not where it is billed, that should matter. I suppose you could have a neighborhood where all the mail was being stolen, and cards were compromised that way, but I think most cards get compromised at the point of sale.

```{python}
fraud = fraud.drop(columns = ["city_pop"])
```

I hate python

```{python}
fraud = fraud.drop(columns = ["merch_lat", "merch_long"])
```

# 6. Final preparation for modeling

Next, I plot the correlation plot for the dataset. Highly correlated variables can cause problems for some fitting algorithms, again, especially for those coming from statistics. It also gives you a bit of a feel for what might come out of the model fitting. This is also a chance to do one last fact-check. For example, `category` and `amt` are reasonably correlated. The sign isn't particularly important in this case since `category` is arbitrarily ordered.

instructions for lower diagonal plot here https://seaborn.pydata.org/examples/many_pairwise_correlations.html

```{python}
#| label: correlation-graph
#Code Block 27: examining correlation between variables 
plt.clf()
sns.heatmap(fraud.corr(numeric_only=True), cmap="YlGnBu", annot=True)
plt.show()



```

```{python}
#https://seaborn.pydata.org/examples/many_pairwise_correlations.html

# Compute the correlation matrix
corr = fraud.corr(numeric_only=True)

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap="YlGnBu", vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot=True)
#plt.xticks(rotation=45)
plt.yticks(rotation=0)
#plt.tight_layout()
plt.show()
```

FIX UP THE LABELS. THE DIAG IS NOT PLOTTED BUT I STILL HAVE THE LABEL. TIGHT LAYOUT NOT WORKING

And take one last look at the data and make sure I have the variables I expect.

```{python}
#| label: final-check-of-data
# Code Block 29: Viewing Final Fraud Dataset
fraud.info()

```

Drop new category

```{python}
fraud = fraud.drop("category_new", axis = 1)
fraud.info()
```

# 7. Finding a high performing model

(https://towardsdatascience.com/quickly-test-multiple-models-a98477476f0)

I'm planning to study the following models and methods of handling imbalanced class problems.

Explore different classification models

1.  logistic regression

2.  elastic net logistic regression

3.  lightgbm

4.  random forest

Explore different method of handling imbalanced class problems

1.  do nothing

2.  SMOTE

3.  ROSE

4.  downsample

This ends up being 4 x 4 different fits, and keeping track of all the combinations can become difficult. In R, the tidymodels package has a function workflow_set that will create all the combinations and workflow_map to run all the fitting procedures. In Python, the functionality of pipeline combined with some loops seem to be the method of choice for handling multiple models.

## 7.1 Loading all the modules

```{python}

from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder


from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

#from xgboost import XGBClassifier
from sklearn import model_selection
from sklearn.utils import class_weight
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import balanced_accuracy_score

from sklearn.model_selection import train_test_split
```

## 7.2 Defining the Features and Target

First define the features and the target. In R, you connect the dependent and independent variables via a formula, but in Python, you separate them into two different dataframes. Using an uppercase X for the features and a lowercase y for the target seems to be a common convention.

```{python}
X = fraud.drop("is_fraud", axis = 1) # Features
y = fraud["is_fraud"] # Target variable
```

## 7.3 Defining the preprocessing steps.

Now I setup the preprocessing recipe. Here, you need to separate out the columns by type and apply the preprocessing steps to the different batches of of columns. In Tidymodels, you build a single recipe, but you can specify at each step which columns or types of columns you apply the preprocessing to. I'm handling day, hour as numerical. They are really ordinal or circular. Cite paper.

I have numeric features (log_amt, day, hour) and categoric features (category)

```{python}

numeric_features=list(X.select_dtypes('number').columns)
categorical_features=list(X.select_dtypes('category').columns)

```

Now define how I want to process each type.

```{python}

numeric_preprocessor = Pipeline(
    steps=[
        ("imputation_mean", SimpleImputer(missing_values=np.nan, strategy="mean")),
        ("scaler", StandardScaler()),
    ]
)

categorical_preprocessor = Pipeline(
    steps=[
        (
            "imputation_constant",
            SimpleImputer(fill_value="missing", strategy="constant"),
        ),
        ("onehot", OneHotEncoder(handle_unknown="ignore")),
    ]
)

preprocessor = ColumnTransformer(
    [
        ("categorical", categorical_preprocessor, categorical_features),
        ("numerical", numeric_preprocessor, numeric_features),
    ]
)

 

```

## 7.4 Now add a classifier

```{python}
pipe = make_pipeline(preprocessor, RandomForestClassifier())
#pipe = make_pipeline(preprocessor, LogisticRegression(max_iter=500))
pipe  # click on the diagram below to see the details of each step

```

This code is copeid from scikitlearn itself https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_pipeline_display.html#displaying-a-pipeline-chaining-multiple-preprocessing-steps-classifier

## 7.5 Now split and stratify the data.

```{python}
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.25, random_state=8675309 )
```

## 7.6 Test for a Single Model

Lastly, I create a metrics set in Code Block 33. Accuracy is generally a terrible metric for highly imbalanced problems; the model can achieve high accuracy by assigning everything to the majority class. Alternate metrics like [sensitivity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) or [j-index](https://en.wikipedia.org/wiki/Youden%27s_J_statistic) are better choices for the imbalanced class situation.

```{python}
pipe.fit(X_train, y_train)  # apply scaling on training data
y_pred = pipe.predict(X_test)


bal_acc=balanced_accuracy_score(y_test,y_pred, adjusted=True )
bal_acc
```

## 7.7 Set Up a Batch Process

### Define the classifiers.

```{python}
#| label: script-workflowset
classifiers = [KNeighborsClassifier(), RandomForestClassifier(), LogisticRegression()]
```

### Iterate Through

```{python}
for classifier in classifiers:
  pipe_iter = Pipeline(steps=[("Preprocessor", preprocessor), ("Classifier", classifier)])
  pipe_iter.fit(X_train, y_train)
  print(classifier)
  print("J Index: ")
  balanced_accuracy_score(y_test, pipe_iter.predict(X_test), adjusted=True )
```

## 7.8 Modify Batch to use CV

```{python}


for classifier in classifiers:
  pipe_iter = Pipeline(steps=[("Preprocessor", preprocessor), ("Classifier", classifier)])
  kfold = model_selection.KFold(n_splits=3, shuffle=True, random_state=90210)
  cv_results = model_selection.cross_validate(pipe_iter, X_train, y_train, cv=kfold, scoring=scoring, verbose = 3)
  cv_results.fit(X_train, y_train)
 # print(classifier)
  #print("J Index: ")
 # balanced_accuracy_score(y_test, cv_results.predict(X_test), adjusted=True )


```
